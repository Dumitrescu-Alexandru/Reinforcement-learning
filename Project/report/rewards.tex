The rewards are important in a RL framework as it sends the signals to the model that allow it to move towards an optimal policy. The particular set up we have we get very sparse rewards, when the agent wins after an episode it gets a reward of $+10$ and if it loses then it gets $-10$. As we have no access to the internal dynamics of the network we cannot engineer any features based on our knowledge of the domain of Pong. 